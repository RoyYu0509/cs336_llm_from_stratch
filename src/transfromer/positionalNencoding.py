import torch
from jaxtyping import Float, Array
from torch import Tensor
from einops import rearrange, reduce, repeat, einsum
import torch.nn as nn


class RoPE:
    """
    A R^{2x2} Rotation Matrix
    """
    def __init__(self, 
                 theta: float, 
                 d_k: int,
                 device=None
        ):
        """
        Create the rotation matrix
        """
        self.d_k = d_k
        self.device = device
        self.theta = torch.tensor(theta, device=device, dtype = torch.float32)
    
    def getRotMat(self, i:int, k:int):
        theta_ik = i / (self.theta.clone()**((2*k)/self.d_k))
        theta_ik = theta_ik.to(self.device)
        _cos = torch.cos(theta_ik)
        _sin = torch.sin(theta_ik)
        
        rotMat_ik: Float[Tensor, "out_vec in_vec"]
        rotMat_ik = torch.tensor(
            [
             [_cos, -_sin],
             [_sin,  _cos]
            ],
            dtype=torch.float32
        )
        return rotMat_ik


class PosEncod(nn.Module):
    def __init__(self, theta: float, d_k: int, max_seq_len: int, device=None):
        """
        Construct the RoPE module and create buffers if needed.
        
        Parameters:
            - theta: float      Constant value for computing frequency
            - d_k: int          Dimension of the token vectors.
            - max_seq_len: int  Maximum sequence length of our input tokens.
        """
        super().__init__()
        self.d_k = d_k
        self.rot_dim = d_k if d_k % 2 == 0 else d_k + 1

        # Buffer the rotation matrix for reuse
        rotations: Float[Tensor, "max_seq_l half_d out_vec in_vec"]
        rotations = torch.stack(
            [
                torch.stack(
                    [
                        RoPE(theta, self.rot_dim).getRotMat(pos, k) for k in range(0, self.rot_dim // 2)
                    ],
                    dim=0,
                )
                for pos in range(max_seq_len)
            ],
            dim=0,
        )
        self.register_buffer("Rs", rotations.to(device=device), persistent=False)
    
    def forward(self, x: torch.Tensor, token_positions: torch.Tensor) -> torch.Tensor:
        """
        Process an in_vecput tensor of shape (..., seq_len, d_k) and 
        return a tensor of the same shape.

        Parameters:
            - x: the input batch data
            - token_positions: (..., seq_len) specifying the token positions of x along the sequence dimension.
        """
        device = x.device
        token_positions = token_positions.to(device)
        # Pad a dummy dimension if the d_k is odd
        d = self.rot_dim
        is_odd_d = (self.d_k % 2 == 1)
        if is_odd_d:
            pad = torch.zeros(*x.shape[:-1], 1, device=device, dtype=x.dtype)
            x = torch.cat([x, pad], dim=-1)
        seq_len = x.shape[-2]
        position = token_positions[..., :seq_len]

        # Get the Rotation Matrix
        rope_cache = self.Rs
        if rope_cache.device != device:
            rope_cache = rope_cache.to(device)
        Rs = rope_cache[position].to(x.dtype)
            
        # Unpack x into several 2-dim vec
        reshape_x = rearrange(x, "... seq (half_d in_vec) -> ... seq half_d in_vec", half_d = d//2, in_vec = 2)
        # print(f"Shape of Rs: {Rs.shape}")
        # print(f"Shape of reshape_x: {reshape_x.shape}")
        Rx = einsum(Rs, reshape_x, "... seq half_d out_vec in_vec, ... seq half_d in_vec -> ... seq half_d out_vec")
        reshape_Rx = rearrange(Rx, "... seq half_d out_vec -> ... seq (half_d out_vec)")
        # print(reshape_Rx.shape, x.shape)
        if is_odd_d:
            return reshape_Rx[..., :self.d_k]
        else:
            return reshape_Rx
    

        
    
